domain: OVERRIDE_VIA_APPSET
clusterName: OVERRIDE_VIA_APPSET

seaweedfs:
  global:
    createClusterRole: true
    monitoring:
      enabled: true
    # if enabled will use global.replicationPlacment and override master & filer defaultReplicaPlacement config
    enableReplication: true
    #  replication type is XYZ:
    # X number of replica in other data centers
    # Y number of replica in other racks in the same data center
    # Z number of replica in other servers in the same rack
    replicationPlacement: "001"

    extraEnvironmentVars:
      WEED_CLUSTER_DEFAULT: "sw"
      WEED_CLUSTER_SW_MASTER: "seaweedfs-master.default:9333"
      WEED_CLUSTER_SW_FILER: "seaweedfs-filer-client.default:8888"

  master:
    enabled: true
    #port: 9333
    #grpcPort: 19333
    metricsPort: 9327
    volumeSizeLimitMB: 30000
    # This default is currently overridden globally.
    # defaultReplication: "000"

    #config: |-
    #  # Enter any extra configuration for master.toml here.
    #  # It may be be a multi-line string.

    # You may use ANY storage-class, example with local-path-provisioner
    # Annotations are optional.
    #  data:
    #    type: "persistentVolumeClaim"
    #    size: "24Ti"
    #    storageClass: "local-path-provisioner"
    #    annotations:
    #      "key": "value"
    #
    # You may also spacify an existing claim:
    #  data:
    #    type: "existingClaim"
    #    claimName: "my-pvc"
    #
    # You can also use emptyDir storage:
    #  data:
    #    type: "emptyDir"
    data:
      type: "persistentVolumeClaim"
      size: 10Gi
    logs:
      type: "existingClaim"
      claimName: "data-default-seaweedfs-master-0"

    # Resource requests, limits, etc. for the master cluster placement. This
    # should map directly to the value of the resources field for a PodSpec,
    # formatted as a multi-line string. By default no direct resource request
    # is made.
    resources:
      requests:
        cpu: 200m
        memory: 2Gi
      limits:
        memory: 8Gi

    # Affinity Settings
    # Commenting out or setting as empty the affinity variable, will allow
    # deployment to single node services such as Minikube
    affinity: {}
      #|
      #podAntiAffinity:
      #  requiredDuringSchedulingIgnoredDuringExecution:
      #    - labelSelector:
      #        matchLabels:
      #          app.kubernetes.io/name: {{ template "seaweedfs.name" . }}
      #          app.kubernetes.io/instance: {{ .Release.Name }}
      #          app.kubernetes.io/component: master
      #      topologyKey: kubernetes.io/hostname

    # Configure security context for Pod
    # ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
    # Example:
    # podSecurityContext:
    #   enabled: true
    #   runAsUser: 1000
    #   runAsGroup: 3000
    #   fsGroup: 2000
    podSecurityContext: {}

    # Configure security context for Container
    # ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
    # Example:
    # containerSecurityContext:
    #   enabled: true
    #   runAsUser: 2000
    #   allowPrivilegeEscalation: false
    containerSecurityContext: {}


    ingress:
      # Disabled and done with template instead because this chart assumes nginx :/
      enabled: false

  volume:
    #enabled: true
    #port: 8080
    #grpcPort: 18080
    metricsPort: 9328
    index: leveldb
    replicas: 5
    minFreeSpacePercent: 2
    #compactionMBps: "250"

    # In the 4.0.0 chart these seemed to change and are now failing... 
    # Though could be that my volumes are actually broken (I know they sorta are) and I need a more permissive health check to fix.
    readinessProbe:
      httpGet:
        path: status
    livenessProbe:
      httpGet:
        path: status

    # For each data disk you may use ANY storage-class, example with local-path-provisioner
    # Annotations are optional.
    #  dataDirs:
    #   - name: data:
    #     type: "persistentVolumeClaim"
    #     size: "24Ti"
    #     storageClass: "local-path-provisioner"
    #     annotations:
    #      "key": "value"
    #     maxVolumes: 0  # If set to zero on non-windows OS, the limit will be auto configured. (default "7")
    #
    # You may also spacify an existing claim:
    #   - name: data
    #     type: "existingClaim"
    #     claimName: "my-pvc"
    #     maxVolumes: 0  # If set to zero on non-windows OS, the limit will be auto configured. (default "7")
    #
    # You can also use emptyDir storage:
    #   - name: data
    #     type: "emptyDir"
    #     maxVolumes: 0  # If set to zero on non-windows OS, the limit will be auto configured. (default "7")
    dataDirs:
      - name: drive
        type: "custom"
        maxVolumes: 0

    extraVolumeMounts: |
      - name: drive
        mountPath: /drive
        subPathExpr: $(POD_NAME)
    extraVolumes: |
      - name: drive
        hostPath: 
          path: /var/mnt/

    # idx can be defined by:
    #
    # idx:
    #  type: "hostPath"
    #  hostPathPrefix: /ssd
    #
    # or
    #
    # idx:
    #  type: "persistentVolumeClaim"
    #  size: "20Gi"
    #  storageClass: "local-path-provisioner"
    #
    # or
    #
    # idx:
    #   type: "existingClaim"
    #   claimName: "myClaim"
    #
    # or
    #
    # idx:
    #  type: "emptyDir"

    # same applies to "logs"

    idx:
      type: "persistentVolumeClaim"
      size: "10Gi"
      mountOptions:
        - direct
        - dirsync

    logs: 
      type: "persistentVolumeClaim"
      size: "10Gi"

    ingress:
      enabled: true
      annotations:
        traefik.ingress.kubernetes.io/router.entrypoints: seaweedvolume

    # Volume server's rack name
    rack: infra.spencerslab.com

    # Volume server's data center name
    dataCenter: spencerslab.com

    # extraEnvVars is a list of extra enviroment variables to set with the stateful set.
    extraEnvironmentVars:
      PUBLICURL: "$(POD_NAME).infra.spencerslab.com:8080"

    # Redirect moved or non-local volumes. (default proxy)
    #readMode: proxy

    # Affinity Settings
    # Commenting out or setting as empty the affinity variable, will allow
    # deployment to single node services such as Minikube
    affinity: {} #|
      #podAntiAffinity:
      #  requiredDuringSchedulingIgnoredDuringExecution:
      #    - labelSelector:
      #        matchLabels:
      #          app.kubernetes.io/name: {{ template "seaweedfs.name" . }}
      #          app.kubernetes.io/instance: {{ .Release.Name }}
      #          app.kubernetes.io/component: volume
      #      topologyKey: kubernetes.io/hostname

    # Resource requests, limits, etc. for the server cluster placement. This
    # should map directly to the value of the resources field for a PodSpec,
    # formatted as a multi-line string. By default no direct resource request
    # is made.
    resources: {}

    # Configure security context for Pod
    # ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
    # Example:
    podSecurityContext:
      enabled: true
      runAsUser: 2000
      runAsGroup: 2000
      fsGroup: 2000

    # Configure security context for Container
    # ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
    # Example:
    # containerSecurityContext:
    #   enabled: true
    #   runAsUser: 2000
    #   allowPrivilegeEscalation: false
    containerSecurityContext: {}
    
  filer:
    enabled: true
    replicas: 5
    #port: 8888
    #grpcPort: 18888
    metricsPort: 9327
    #encryptVolumeData: false

    # This default is currently overridden globally.
    #  replication type is XYZ:
    # X number of replica in other data centers
    # Y number of replica in other racks in the same data center
    # Z number of replica in other servers in the same rack
    #defaultReplicaPlacement: "000"

    # encrypt data on volume servers
    encryptVolumeData: false

    # Disable http request, only gRpc operations are allowed
    # TODO: This has to be disabled because the health check is hard coded for HTTP.
    disableHttp: false

    # used to configure livenessProbe on filer containers
    livenessProbe:
      enabled: false
      grpc:
        port: 18888

    # used to configure readinessProbe on filer containers
    readinessProbe:
      enabled: false
      grpc:
        port: 18888

    # You may use ANY storage-class, example with local-path-provisioner
    # Annotations are optional.
    #  data:
    #    type: "persistentVolumeClaim"
    #    size: "24Ti"
    #    storageClass: "local-path-provisioner"
    #    annotations:
    #      "key": "value"
    #
    # You may also specify an existing claim:
    #  data:
    #    type: "existingClaim"
    #    claimName: "my-pvc"
    #
    # You can also use emptyDir storage:
    #  data:
    #    type: "emptyDir"
    # or hostpath?: 
      #type: "hostPath"
      #hostPathPrefix: /storage
    data:
      type: "persistentVolumeClaim"
      size: "10Gi"
    logs:
      type: "existingClaim"
      claimName: "data-filer-seaweedfs-filer-0"

    # Affinity Settings
    # Commenting out or setting as empty the affinity variable, will allow
    # deployment to single node services such as Minikube
    affinity: {} #|
      #podAntiAffinity:
      #  requiredDuringSchedulingIgnoredDuringExecution:
      #    - labelSelector:
      #        matchLabels:
      #          app.kubernetes.io/name: {{ template "seaweedfs.name" . }}
      #          app.kubernetes.io/instance: {{ .Release.Name }}
      #          app.kubernetes.io/component: filer
      #      topologyKey: kubernetes.io/hostname

    # Resource requests, limits, etc. for the server cluster placement. This
    # should map directly to the value of the resources field for a PodSpec,
    # formatted as a multi-line string. By default no direct resource request
    # is made.
    resources: {}

    # nodeSelector labels for server pod assignment, formatted as a muli-line string.
    # ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector
    # Example:
    #nodeSelector: |
    #  kubernetes.io/arch: amd64
    # nodeSelector: |
    #   sw-backend: "true"

    # Configure security context for Pod
    # ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
    # Example:
    # podSecurityContext:
    #   enabled: true
    #   runAsUser: 1000
    #   runAsGroup: 3000
    #   fsGroup: 2000
    podSecurityContext: {}

    # Configure security context for Container
    # ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
    # Example:
    # containerSecurityContext:
    #   enabled: true
    #   runAsUser: 2000
    #   allowPrivilegeEscalation: false
    containerSecurityContext: {}

    ingress:
      enabled: false

    # extraEnvVars is a list of extra enviroment variables to set with the stateful set.
    extraEnvironmentVars:
      WEED_LEVELDB2_ENABLED: "true"
      # with http DELETE, by default the filer would check whether a folder is empty.
      # recursive_delete will delete all sub folders and files, similar to "rm -Rf"
      WEED_FILER_OPTIONS_RECURSIVE_DELETE: "true"
      # directories under this folder will be automatically creating a separate bucket
      WEED_FILER_BUCKETS_FOLDER: "/buckets"

    # secret env variables
    secretExtraEnvironmentVars: {}
        # WEED_POSTGRES_USERNAME:
        #   secretKeyRef:
        #     name: postgres-credentials
        #     key: username
        # WEED_POSTGRES_PASSWORD:
        #   secretKeyRef:
        #     name: postgres-credentials
        #     key: password
  s3:
    enabled: true
    allowEmptyFolder: true
    # Suffix of the host name, {bucket}.{domainName}
    domainName: "s3.spencerslab.com"
    ingress:
      enabled: false
    metricsPort: 9327

    # Resource requests, limits, etc. for the server cluster placement. This
    # should map directly to the value of the resources field for a PodSpec,
    # formatted as a multi-line string. By default no direct resource request
    # is made.
    resources: {}

    # nodeSelector labels for server pod assignment, formatted as a muli-line string.
    # ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector
    # Example:
    #nodeSelector: |
    #  kubernetes.io/arch: amd64
    # nodeSelector: |
    #   sw-backend: "true"

    # used to assign priority to server pods
    # ref: https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/
    #priorityClassName: ""

    # used to assign a service account.
    # ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
    #serviceAccountName: ""

    # Configure security context for Pod
    # ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
    # Example:
    # podSecurityContext:
    #   enabled: true
    #   runAsUser: 1000
    #   runAsGroup: 3000
    #   fsGroup: 2000
    podSecurityContext: {}

    # Configure security context for Container
    # ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
    # Example:
    # containerSecurityContext:
    #   enabled: true
    #   runAsUser: 2000
    #   allowPrivilegeEscalation: false
    containerSecurityContext: {}

    # You can also use emptyDir storage:
    #  logs:
    #    type: "emptyDir"
    logs:
      type: "existingClaim"
      claimName: "data-s3-seaweedfs-s3-0"

#seaweedfs-volume-server-0:
#  global:
#    monitoring:
#      enabled: true
#    extraEnvironmentVars:
#      WEED_CLUSTER_DEFAULT: "sw"
#      WEED_CLUSTER_SW_MASTER: "seaweedfs-master.default:9333"
#      WEED_CLUSTER_SW_FILER: "seaweedfs-filer-client.default:8888"
#  master:
#    enabled: false
#  filer:  
#    enabled: false
#    s3: 
#      enabled: false
#
#  volume:
#    #enabled: true
#    #port: 8080
#    #grpcPort: 18080
#    metricsPort: 9328
#    index: leveldb
#    replicas: 1
#
#    # In the 4.0.0 chart these seemed to change and are now failing... 
#    # Though could be that my volumes are actually broken (I know they sorta are) and I need a more permissive health check to fix.
#    readinessProbe:
#      httpGet:
#        path: status
#    livenessProbe:
#      httpGet:
#        path: status
#
#    dataDirs:
#      - name: hdd1
#        type: "hostPath"
#        hostPathPrefix: /var/mnt/drive1
#        maxVolumes: 0
#      - name: hdd2
#        type: "hostPath"
#        hostPathPrefix: /var/mnt/drive2
#        maxVolumes: 0
#
#    idx:
#      type: "persistentVolumeClaim"
#      size: "10Gi"
#
#    logs: 
#      type: "persistentVolumeClaim"
#      size: "2Gi"
#
#    ingress:
#      enabled: false
#
#    # Volume server's rack name
#    rack: infra.spencerslab.com
#
#    # Volume server's data center name
#    dataCenter: spencerslab.com
#
#    # extraEnvVars is a list of extra enviroment variables to set with the stateful set.
#    extraEnvironmentVars:
#      WEED_PUBLICURL: "seaweedfs-volume-0.infra.spencerslab.com:8080"
#      WEED_PUBLIC_URL: "seaweedfs-volume-0.infra.spencerslab.com:8080"
#      # TODO: This doesn't work. Have to add the flag to the chart.
#      #WEED_DISK: backup
#
#    # Configure security context for Pod
#    # ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
#    # Example:
#    podSecurityContext:
#      enabled: true
#      runAsUser: 2000
#      runAsGroup: 2000
#      fsGroup: 2000
